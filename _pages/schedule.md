---
layout: archive
title: "Schedule"
permalink: /schedule/
author_profile: false
---

<!--
We note that Machine Learning is a subject with a lot of very good expertise and tutorials out there. It is best to tap on these resources, as they have good production quality and are more condensed, possibly saving you time. However, we still think in-class lecture is helpful to build better connection with the materials for certain topics.

This class will be _flipped_; i.e., you will be asked to watch videos on YouTube explaining the concepts on your own first (the pre videos), and then after the appropriate tutorial session where staff will guide you through the pertinent exercises and reinforcement activities. Post-tutorial, you will be expected to complete the second half of the videos (the post videos) and complete a set of mastery exercises in Coursemology.  Note that the dates in the _date_ column below are indexed for Mondays (the day of the first class lecture according to the registrar).

To be clear, the dates on this website are just for easy reference, but the authoritative dates will always be in [Coursemology](https://coursemology.org/courses/1870).  Take note of any conflicting deadlines and let us know.

For those who find the pace of the videos too fast or needing a bit more time to digest the materials, we will offer an in-class help session during the lecture slot (i.e., Thursdays 12:00-14:00) on the remaining weeks (Weeks 2-6 and 8-12). This is completely optional (not counting against your workload), and we will not be introducing any material for the help sessions. It is just voluntary help from all of us on the staff. 
-->


<table class="table table-striped">
<thead class="thead-inverse"><tr><th>Date</th><th>Description</th><th>Deadlines</th></tr></thead>
<tbody>
<tr>
  <td><b>NUS Week 01</b><br />Fri, 15 Jan
  </td>
  <td>Administrivia and Regular Expressions, Finite State Automata</td>
<!--  <td>Â· 
-->
  </td>
</tr>
<tr>
  <td><b>Week 02</b><br />22 Jan
  </td>
  <td>
    Words, Spelling Errors, Edit Distance, and N-grams
  </td>
  <td>
  </td>
</tr>
<tr>
  <td><b>Week 03</b><br />29 Jan
  </td>
  <td>
    Part-of-Speech Tagging
  </td>
  <td>
  </td>
</tr>
<tr>
  <td><b>Week 04</b><br />5 Feb
  </td>
  <td>Linear Models and Feed Forward Neural Networks
  </td> 
  <td>
  </td>
</tr>
<tr>
  <td><b>Week 05</b><br />12 Feb
  </td>
  <td>Neural Network Training and Neural Language Models
  </td>
  <td>
  </td>
</tr>
<tr>
  <td><b>Week 06</b><br />14 Sep
  </td>
  <td>Word Embeddings
  </td>
  <td>
  </td>
</tr>
<tr>
  <td><b>Week Recess</b><br />21 Sep
  </td>
  <td>
  </td>
  <td>
  </td>
</tr>
<tr>
  <td><b>Week 07</b><br />28 Sep
  </td>
  <td>Convolutional Neural Networks
  </td>
  <td>
  </td>
</tr>
<tr>
  <td><b>Week 08</b><br />5 Oct
  </td>
  <td>Recurrent Neural Networks
  </td>
  <td>
  </td>
</tr>
<tr>
  <td><b>Week 09</b><br />12 Oct
  </td>
  <td>Sequence to Sequence and Grammars
  </td>
  <td>
  </td>
</tr>
<tr>
  <td><b>Week 10</b><br />19 Oct
  </td>
  <td>Syntactic Parsing
  </td>
  <td>
  </td>
</tr>
<tr>
  <td><b>Week 11</b><br />26 Oct
  </td>
  <td>Statistical and Neural Parsing, and Semantics and First Order Logic
  </td>
  <td>
  </td>
</tr>
<tr>
  <td><b>Week 12</b><br />2 Nov
  </td>
  <td>Semantic Analysis
  </td>
  <td>
  </td>
</tr>
<tr>
  <td><b>Week 13</b><br />9 Nov
  </td>
  <td>Discourse
  </td>
  <td>
  </td>
</tr>
<tr>
  <th><b>Exam Week</b><br />TBA
  </th>
  <th>No Examinations.  100% Continuous Assessment.
  </th>
  <th>
  </th>
</tr>
</tbody></table>

<p><br /></p>

<!--

-->
